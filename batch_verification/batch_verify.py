from collections import defaultdict
from typing import List, Dict
import os

os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = (
    "3"  # disable information and warning from tensorflow
)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import argparse

from matplotlib import pyplot as plt
import onnxruntime as ort
import numpy as np
import jax.numpy as jnp

from src.crown import crown_verifier
from src.mip import mip_verifier
from src.similarity import Similarity
from src.ce_checker import Checker

from util.gurobi_modeling import GurobiModel
from util.scip_modeling import SCIPModel
from util.mip_modeling import Model
from util.read_dataset import extract_network_structure, load_dataset
from util.read_results import get_ce
from util.parameters_networks import NetworksStructure, DataSet
from util.write_vnnlib import (
    write_vnnlib,
    write_vnnlib_join,
    write_vnnlib_meet,
    export_vnnlib,
)
from util.merge_inputs import merge_inputs
from util.options import InputMergedBy, VerificationSolver, Mode
from util.save_results import Results
from util.log import Logger


# * algorithms


# TODO: implement verification algorithm in different ways
def verify(
    solver: VerificationSolver,
    onnx_filename: str,
    vnnlib_filename: str,
    all_images: List[jnp.ndarray],
    mergedtype: InputMergedBy,
    true_label: int,
    epsilon: float,
) -> str:
    """
    Verification algorithm:

    Support: MIP (SCIP, Gurobi), CROWN
    """
    result: str = "UNSAT"
    networks: NetworksStructure = extract_network_structure(
        onnx_file_path=onnx_filename, vnnlib_file_path=vnnlib_filename
    )
    m: SCIPModel | GurobiModel | None = None

    if solver is VerificationSolver.SCIP or solver is VerificationSolver.GUROBI:
        Logger.info(messages=f"Verification Algorithm is MIP solver ({solver})")

        m: SCIPModel | GurobiModel = mip_verifier(
            solver_name=solver, networks=networks
        )
        result = "UNSAT" if m.get_solution_status() == "Infeasible" else "SAT"
    elif solver is VerificationSolver.CROWN:
        Logger.info(messages="Verification Algorithm is CROWN")

        result = crown_verifier(
            onnx_file_path=onnx_filename, vnnlib_file_path=vnnlib_filename
        )

    if result == "UNSAT":
        Logger.info(messages="New template generated!")
        return "UNSAT"
    else:
        # * Testing checker for counter-example found by MIP
        counter_example: List[float] = get_ce(
            solver=solver, networks=networks, filename="./test_cex.txt", m=m
        )
        counter_example: jnp.ndarray = jnp.array(counter_example)

        ce_checker: Checker = Checker(
            all_images=all_images,
            counter_example=counter_example,
            epsilon=epsilon,
        )

        ce_id: int
        res_ce_check: bool
        ce_id, res_ce_check = ce_checker.check()
        Logger.debugging(messages=f"counter example check: {res_ce_check}")

        if res_ce_check is False:
            Logger.error(messages="Counter example is not correct")
            # TODO: divide and conquer
            # TODO: tree based search...
            # ? How to divide the input domain?
            # ? What is the goal?
            # ? What is the termination condition?

            if len(all_images) == 1:
                Logger.error(messages="IMPOSSIBLE")
                return "IMPOSSIBLE"
            
            # * binary search 
            left: List[jnp.ndarray] = all_images[:len(all_images) // 2]
            right: List[jnp.ndarray] = all_images[len(all_images) // 2:]
            verify(solver=solver, 
                    onnx_filename=onnx_filename, 
                    vnnlib_filename=vnnlib_filename, 
                    all_images=left, 
                    mergedtype=mergedtype, 
                    true_label=true_label, 
                    epsilon=epsilon)
            verify(solver=solver, 
                    onnx_filename=onnx_filename, 
                    vnnlib_filename=vnnlib_filename, 
                    all_images=right, 
                    mergedtype=mergedtype, 
                    true_label=true_label, 
                    epsilon=epsilon)
        else:
            Logger.info(
                messages="Counter example from MIP is a real counter example"
            )
            assert ce_id != -1

            Results.add(ce=counter_example, ce_id=ce_id, label=true_label)

            # * updating input domain
            vnnlib_filename: str = merge_inputs(
                all_inputs=all_images,
                removed_inputs=Results.get_unsatisfiable_inputs(label=true_label),
                num_input_dimensions=networks.num_inputs,
                num_output_dimension=networks.num_outputs,
                true_label=true_label,
                epsilon=epsilon,
                mergedtype=mergedtype,
            )
            all_images.remove(ce_id)
            Logger.info(messages="updated the input domain")
            Logger.debugging(
                messages=f"removed: {Results.get_unsatisfiable_inputs(label=true_label)}"
            )
            verify(solver=solver,
                   onnx_filename=onnx_filename,
                   vnnlib_filename=vnnlib_filename,
                   all_images=all_images,
                   mergedtype=mergedtype,
                   true_label=true_label,
                   epsilon=epsilon)

    Logger.info(messages=f"verification result: {result}")

    return result


def debug(solver: VerificationSolver, mergedtype: InputMergedBy) -> str:
    """
    Batch verification algorithm:
        step 0: read the input files (onnx, image files)

        step 1: filter correct classification results from testing dataset.
        step 2: based on each label, separate into different groups.
        step 3: similarity analysis
                - goal: group similar data.
                        based on pre-defined metric.
        step 4: define abstract domain, ð’œ for each cluster.
        step 5: Verify ð’œ -> r.
                    if r is UNSAT: STOP
                    else:
                        divide ð’œ into ð’œ_1, ð’œ_2,..., ð’œ_n
                        back to step 5 to verify each ð’œ_i

    Purpose:
        1. Reduce training data to train robust NN.
        2. Accelerate overall verification process.
        3. When we retrain NN, we have to verify the property again, so this might help to reduce the cost.

    Logger:
        debugging: # * for show the intermediate results.
        info: # * for show the main results.
        error: # * for show the error messages.
    """
    # *  ************************  * #
    # *  step 0. read the input files
    # *  ************************  * #
    Logger.info(messages="step 0: read the input files")
    dataset: DataSet = load_dataset("mnist")
    onnx_filename: str = "./util/benchmarks/onnx/mnist-net_256x2.onnx"
    vnnlib_filename: str = "./util/benchmarks/vnnlib/prop_7_0.03.vnnlib"
    # networks: NetworksStructure = extract_network_structure(onnx_filename, vnnlib_filename)

    # *  ************************  * #
    # *  step 1. filter correct classification results from testing dataset.
    # *  ************************  * #
    Logger.info(
        messages="step 1: filter correct classification results from testing dataset"
    )
    session = ort.InferenceSession(
        onnx_filename, providers=ort.get_available_providers()
    )
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    # *  =======================  * #
    # *  training dataset part
    # *  =======================  * #
    # filter_train_images: List[jnp.ndarray] = []
    # filter_train_labels: List[int] = []
    # num_train_images: int = len(dataset.train_images)
    # Logger.debugging(messages=f"number of images in training dataset: {len(dataset.train_images)}")
    # for data_id in range(num_train_images):
    #     inference_result = session.run([output_name],
    #                                    {input_name: dataset.train_images[data_id].astype(jnp.float32).reshape(1, dataset.num_pixels, 1)})[0]
    #     inference_label = jnp.argmax(inference_result)
    #     true_label = dataset.train_labels[data_id]
    #     if inference_label == true_label:
    #         filter_train_images.append(dataset.train_images[data_id])
    #         filter_train_labels.append(true_label)
    # Logger.debugging(messages=f"filter_train_images: {len(filter_train_images)}")
    # Logger.debugging(messages=f"filter_train_labels: {len(filter_train_labels)}")

    # *  =======================  * #
    # *  testing dataset part
    # *  =======================  * #
    filterd_test_images: List[jnp.ndarray] = []
    filterd_test_labels: List[int] = []
    num_test_images: int = len(dataset.test_images)
    Logger.debugging(messages=f"number of images in testing dataset: {num_test_images}")
    for data_id in range(num_test_images):
        inference_result = session.run(
            [output_name],
            {
                input_name: dataset.test_images[data_id]
                .astype(jnp.float32)
                .reshape(1, dataset.num_pixels, 1)
            },
        )[0]
        inference_label = jnp.argmax(inference_result)
        true_label = dataset.test_labels[data_id]
        if inference_label == true_label:
            filterd_test_images.append(dataset.test_images[data_id])
            filterd_test_labels.append(true_label)

    Logger.debugging(messages=f"filterd_test_images: {len(filterd_test_images)}")
    Logger.debugging(messages=f"filterd_test_labels: {len(filterd_test_labels)}")
    Logger.debugging(
        messages=f"test accuracy: {len(filterd_test_images) / num_test_images}"
    )

    # *  ************************  * #
    # *  step 2. based on each label, separate into different groups.
    # *  ************************  * #
    distribution_filtered_test_labels: Dict[int, List[jnp.ndarray]] = {}
    for label in range(dataset.num_labels):
        distribution_filtered_test_labels[label] = []
    for index, label in enumerate(filterd_test_labels):
        distribution_filtered_test_labels[label].append(filterd_test_images[index])
    for label in range(dataset.num_labels):
        Logger.debugging(
            messages=f"label: {label}, number of images: {len(distribution_filtered_test_labels[label])}"
        )

    # *  ************************  * #
    # *  step 3. similarity analysis
    # *  ************************  * #
    type_of_property: InputMergedBy = InputMergedBy.JOIN
    vnnlib_filename: str = ""
    test_true_label: int = 0  # YES: 0,    Y3(1)
    epsilon: float = 0.03
    count: int = 0
    distance_matrix: jnp.ndarray
    Logger.debugging(
        messages=f"number of testing images: {len(distribution_filtered_test_labels[test_true_label])}"
    )
    distance_matrix = Similarity.generate_distance_matrix(
        all_data=distribution_filtered_test_labels[test_true_label],
        distance_type="l2",
        chunk_size=100,
    )  # ! out of memory

    # * find the similar data
    similarity_data: List[int] = Similarity.greedy(
        distance_matrix=distance_matrix, num_clusters=2
    )
    Logger.debugging(messages=f"similarity_data: {similarity_data}")

    # TODO: Test merge abstract domain if possible
    test_set_inputs: List[int] = similarity_data
    # test_set_inputs: List[int] = [similarity_data[0], similarity_data[1], similarity_data[2], similarity_data[3]]
    # test_set_inputs: List[int] = [similarity_data[3], similarity_data[len(similarity_data) - 1]]
    for id in test_set_inputs:
        if type_of_property is InputMergedBy.JOIN:
            if id == test_set_inputs[0]:
                vnnlib_filename: str = write_vnnlib(
                    data=distribution_filtered_test_labels[test_true_label][id],
                    data_id=id,
                    num_classes=dataset.num_labels,
                    true_label=test_true_label,
                    epsilon=epsilon,
                )
            else:
                os.remove(vnnlib_filename)
                vnnlib_filename: str = write_vnnlib_join(
                    networks=networks,
                    data=distribution_filtered_test_labels[test_true_label][id],
                    data_id=id,
                    num_classes=dataset.num_labels,
                    true_label=test_true_label,
                    epsilon=epsilon,
                )
        elif type_of_property is InputMergedBy.MEET:
            if id == test_set_inputs[0]:
                # if True:
                vnnlib_filename: str = write_vnnlib(
                    data=distribution_filtered_test_labels[test_true_label][id],
                    data_id=id,
                    num_classes=dataset.num_labels,
                    true_label=test_true_label,
                    epsilon=epsilon,
                )
            else:
                os.remove(vnnlib_filename)
                vnnlib_filename: str = write_vnnlib_meet(
                    networks=networks,
                    data=distribution_filtered_test_labels[test_true_label][id],
                    data_id=id,
                    num_classes=dataset.num_labels,
                    true_label=test_true_label,
                    epsilon=epsilon,
                )

        # * update networks by new vnnlib
        networks: NetworksStructure = extract_network_structure(
            onnx_filename, vnnlib_filename
        )

    Logger.info(messages="start verifying ...")
    result: str = verify(
        solver=solver,
        onnx_filename=onnx_filename,
        vnnlib_filename=vnnlib_filename,
        all_images=distribution_filtered_test_labels[test_true_label],
        mergedtype=mergedtype,
        true_label=test_true_label,
        epsilon=epsilon,
    )

    # m: SCIPModel | GurobiModel = mip_verifier(solver_name=solver, networks=networks)
    # counter_example: List[float] = []
    # if m.get_solution_status() == "Infeasible":
    #     Logger.info(messages="UNSAT")
    # else:
    #     for k, Nk in enumerate(networks.layer_to_layer):
    #         if k == 0:
    #             for nk in range(Nk[0]):
    #                 name: str = f"x_{k}_{nk}"
    #                 variable = m.solver.continue_variables[name]
    #                 counter_example.append(m.get_primal_solution(variable))
    #         else:
    #             break
    #     Logger.info(messages="SAT")

    # # plt.imshow(distribution_filtered_test_labels[test_true_label][id].reshape(28, 28), cmap='gray')
    # counter_example_image = np.array(counter_example).reshape(28, 28)
    # plt.imshow(counter_example_image, cmap='gray')
    # plt.show()

    # TODO: implement cluster analysis

    # *  ************************  * #
    # *  step 4. define abstract domain, ð’œ for each cluster.
    # *  ************************  * #
    # TODO: implement abstract domain transformer for interval, zonotope.

    # *  ************************  * #
    # *  step 5. Verify ð’œ -> r.
    # *     if r is UNSAT: STOP
    # *     else:
    # *         divide ð’œ into ð’œ_1, ð’œ_2,..., ð’œ_n
    # *         back to step 5 to verify each ð’œ_i
    # *  ************************  * #
    # TODO: implement verification algorithm

    return result


def release(solver: VerificationSolver, mergedtype: InputMergedBy) -> str:
    """
    Batch verification algorithm:
        step 0: read the input files (onnx, image files)

        step 1: filter correct classification results from testing dataset.
        step 2: based on each label, separate into different groups.
        step 3: cluster analysis
                - goal: group similar data.
                        based on pre-defined metric.
        step 4: define abstract domain, ð’œ for each cluster.
        step 5: Verify ð’œ -> r.
                    if r is UNSAT: STOP
                    else:
                        divide ð’œ into ð’œ_1, ð’œ_2,..., ð’œ_n
                        back to step 5 to verify each ð’œ_i

    Purpose:
        1. Reduce training data to train robust NN.
        2. Accelerate overall verification process.
        3. When we retrain NN, we have to verify the property again, so this might help to reduce the cost.
    """
    # *  ************************  * #
    # *  step 0. read the input files
    # *  ************************  * #
    Logger.info(messages="step 0: read the input files")
    dataset: DataSet = load_dataset("mnist")
    onnx_filename: str = "./util/benchmarks/onnx/mnist-net_256x2.onnx"
    vnnlib_filename: str = "./util/benchmarks/vnnlib/prop_7_0.03.vnnlib"
    # networks: NetworksStructure = extract_network_structure(onnx_filename, vnnlib_filename)

    # *  ************************  * #
    # *  step 1. filter correct classification results from testing dataset.
    # *  ************************  * #
    Logger.info(
        messages="step 1: filter correct classification results from testing dataset"
    )
    session = ort.InferenceSession(
        onnx_filename, providers=ort.get_available_providers()
    )
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name

    # *  =======================  * #
    # *  training dataset part
    # *  =======================  * #
    # filter_train_images: List[jnp.ndarray] = []
    # filter_train_labels: List[int] = []
    # num_train_images: int = len(dataset.train_images)
    # print("number of images in training dataset: ", num_train_images)
    # for data_id in range(num_train_images):
    #     inference_result = session.run([output_name],
    #                                    {input_name: dataset.train_images[data_id].astype(jnp.float32).reshape(1, dataset.num_pixels, 1)})[0]
    #     inference_label = jnp.argmax(inference_result)
    #     true_label = dataset.train_labels[data_id]
    #     if inference_label == true_label:
    #         filter_train_images.append(dataset.train_images[data_id])
    #         filter_train_labels.append(true_label)
    # print("filter_train_images: ", len(filter_train_images))
    # print("filter_train_labels: ", len(filter_train_labels))

    # *  =======================  * #
    # *  testing dataset part
    # *  =======================  * #
    filterd_test_images: List[jnp.ndarray] = []
    filterd_test_labels: List[int] = []
    num_test_images: int = len(dataset.test_images)
    Logger.debugging(messages=f"number of images in testing dataset: {num_test_images}")
    for data_id in range(num_test_images):
        inference_result = session.run(
            [output_name],
            {
                input_name: dataset.test_images[data_id]
                .astype(jnp.float32)
                .reshape(1, dataset.num_pixels, 1)
            },
        )[0]
        inference_label = jnp.argmax(inference_result)
        true_label = dataset.test_labels[data_id]
        if inference_label == true_label:
            filterd_test_images.append(dataset.test_images[data_id])
            filterd_test_labels.append(true_label)

    Logger.debugging(messages=f"filterd_test_images: {len(filterd_test_images)}")
    Logger.debugging(messages=f"filterd_test_labels: {len(filterd_test_labels)}")
    Logger.debugging(
        messages=f"test accuracy: {len(filterd_test_images) / num_test_images}"
    )

    # *  ************************  * #
    # *  step 2. based on each label, separate into different groups.
    # *  ************************  * #
    distribution_filtered_test_labels: Dict[int, List[jnp.ndarray]] = (
        {}
    )  # * key: label, value: images
    for label in range(dataset.num_labels):
        distribution_filtered_test_labels[label] = []
    for index, label in enumerate(filterd_test_labels):
        distribution_filtered_test_labels[label].append(filterd_test_images[index])
    for label in range(dataset.num_labels):
        Logger.debugging(
            messages=f"label: {label}, number of images: {len(distribution_filtered_test_labels[label])}"
        )

    # *  ************************  * #
    # *  step 3. similarity analysis
    # *  ************************  * #
    vnnlib_filename: str = ""
    test_true_label: int = 0  # YES: 0,    Y3(1)
    epsilon: float = 0.03
    count: int = 0
    distance_matrix: jnp.ndarray
    Logger.debugging(
        messages=f"number of testing images: {len(distribution_filtered_test_labels[test_true_label])}"
    )
    distance_matrix = Similarity.generate_distance_matrix(
        all_data=distribution_filtered_test_labels[test_true_label],
        distance_type="l2",
        chunk_size=100,
    )

    # * find the similar data
    similarity_data: List[int] = Similarity.greedy(
        distance_matrix=distance_matrix, num_clusters=2
    )

    # * merge abstract domain if possible
    # * update networks by new vnnlib
    vnnlib_filename: str = merge_inputs(
        all_inputs=distribution_filtered_test_labels[test_true_label],
        removed_inputs=[],
        num_input_dimensions=dataset.num_pixels,
        num_output_dimension=dataset.num_labels,
        true_label=test_true_label,
        epsilon=epsilon,
        mergedtype=mergedtype,
    )

    Logger.info(messages="start verifying ...")
    result: str = verify(
        solver=solver,
        onnx_filename=onnx_filename,
        vnnlib_filename=vnnlib_filename,
        all_images=distribution_filtered_test_labels[test_true_label],
        mergedtype=mergedtype,
        true_label=test_true_label,
        epsilon=epsilon,
    )

    # m: SCIPModel | GurobiModel = mip_verifier(solver_name=solver, networks=networks)
    # counter_example: List[float] = []
    # if m.get_solution_status() == "Infeasible":
    #     Logger.info(messages="UNSAT")
    # else:
    #     for k, Nk in enumerate(networks.layer_to_layer):
    #         if k == 0:
    #             for nk in range(Nk[0]):
    #                 name: str = f"x_{k}_{nk}"
    #                 variable = m.solver.continue_variables[name]
    #                 counter_example.append(m.get_primal_solution(variable))
    #         else:
    #             break
    #     Logger.info(messages="SAT")

    # # plt.imshow(distribution_filtered_test_labels[test_true_label][id].reshape(28, 28), cmap='gray')
    # counter_example_image = np.array(counter_example).reshape(28, 28)
    # plt.imshow(counter_example_image, cmap='gray')
    # plt.show()

    # TODO: implement cluster analysis

    # *  ************************  * #
    # *  step 4. define abstract domain, ð’œ for each cluster.
    # *  ************************  * #
    # TODO: implement abstract domain transformer for interval, zonotope.

    # *  ************************  * #
    # *  step 5. Verify ð’œ -> r.
    # *     if r is UNSAT: STOP
    # *     else:
    # *         divide ð’œ into ð’œ_1, ð’œ_2,..., ð’œ_n
    # *         back to step 5 to verify each ð’œ_i
    # *  ************************  * #
    # TODO: implement verification algorithm

    return result


def main(
    mode: Mode = Mode.DEBUG,
    solver: VerificationSolver = VerificationSolver.SCIP,
    mergedtype: InputMergedBy = InputMergedBy.JOIN,
) -> str:
    Logger.initialize(filename="log.txt", with_log_file=False)
    Logger.info(messages="batch verification is starting...")

    if mode is Mode.DEBUG:
        Logger.info(messages="debug mode is enabled")
        return debug(solver=solver, mergedtype=mergedtype)
    elif mode is Mode.RELEASE:
        Logger.info(messages="release mode is enabled")
        return release(solver=solver, mergedtype=mergedtype)
    else:
        Logger.error(messages="mode is not supported")

    return "main version"


if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    parser.add_argument("--mode", type=str, default="debug")
    parser.add_argument("--solver", type=str, default="scip")
    parser.add_argument("--mergedtype", type=str, default="join")

    args = parser.parse_args()
    mode: Mode = Mode(args.mode)
    solver: VerificationSolver = VerificationSolver(args.solver)
    mergedtype: InputMergedBy = InputMergedBy(args.mergedtype)

    main(mode=mode, solver=solver, mergedtype=mergedtype)
